{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWA2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-3])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.AWA2DataLoader import AWA2DataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path_AoA,save_NFS\n",
    "#from core.Scheduler import Scheduler\n",
    "import importlib\n",
    "import pdb\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.helper_func\n",
    "reload(core.helper_func)\n",
    "from core.helper_func import next_unseen_batch,compute_mean_real_unseen,compute_mean_real_seen,compute_generated_quality,evaluate_quality,evaluate_quality_omni,selective_sample,eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(critic_iter=5, gen_hidden=1024, lambda1=10, normalize_V=True, nz=312)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--critic_iter', type=int, default=5, help='critic iteration, following WGAN-GP')\n",
    "parser.add_argument('--lambda1', type=float, default=10, help='gradient penalty regularizer, following WGAN-GP')\n",
    "parser.add_argument('--nz', type=int, default=312, help='size of the latent z vector')\n",
    "parser.add_argument('--normalize_V', type=bool, default=True, help='normalize_V')\n",
    "parser.add_argument('--gen_hidden', type=int, default=1024, help='gen_hidden')\n",
    "opt = parser.parse_known_args()[0] #omit unknown arguments\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path  = save_NFS+'results/Release_AWA_Composer_Normalize_V/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "AWA2\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Balance dataloader\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/AWA2/feature_map_ResNet_101_AWA2.hdf5\n",
      "Expert Attr\n",
      "threshold at zero attribute with negative value\n",
      "Finish loading data in  165.109753\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "idx_GPU = 2\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")\n",
    "#%%\n",
    "dataloader = AWA2DataLoader(NFS_path_AoA,device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain DAZLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomize seed 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = F.normalize(torch.tensor(att)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "normalize V\n",
      "normalize F\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print('Randomize seed {}'.format(seed))\n",
    "#%%\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att#dataloader.normalize_att#\n",
    "normalize_att = dataloader.normalize_att\n",
    "#assert (att.min().item() == 0 and att.max().item() == 1)\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_1 = 0.0\n",
    "lambda_2 = 0.0\n",
    "lambda_3 = 0.0\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = True\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1#unseenclass.size(0)/(seenclass.size(0)+unseenclass.size(0))\n",
    "report_interval = niters//nepoches#10000//batch_size#\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=False,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_1 0.0\n",
      "lambda_2 0.0\n",
      "lambda_3 0.0\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.0 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.#0.#\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "#%%\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_1 {}'.format(lambda_1))\n",
    "print('lambda_2 {}'.format(lambda_2))\n",
    "print('lambda_3 {}'.format(lambda_3))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_phase_1 = save_path+'phase_1_checkpoint/'\n",
    "run_phase_1 = False\n",
    "if not os.path.isdir(save_dir_phase_1):\n",
    "    run_phase_1 = True\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.88995361328125 acc_novel 0.2082993984222412 H 0.3375848739675165\n",
      "acc_zs 0.6440294981002808\n"
     ]
    }
   ],
   "source": [
    "if run_phase_1:\n",
    "    #%% phase 1 convergence of attention model\n",
    "    for i in range(0,2000):\n",
    "        model.train()\n",
    "        optimizer_m.zero_grad()\n",
    "        batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "        out_package = model(batch_feature)\n",
    "\n",
    "        in_package = out_package\n",
    "        in_package['batch_label'] = batch_label\n",
    "        \n",
    "        out_package=model.compute_loss(in_package)\n",
    "        loss,loss_CE,loss_w2v,loss_prune,loss_pmp = out_package['loss'],out_package['loss_CE'],out_package['loss_w2v'],out_package['loss_prune'],out_package['loss_pmp']\n",
    "        entropy = out_package['entropy']\n",
    "        entropy_A_p = out_package['entropy_A_p']\n",
    "        loss.backward()\n",
    "        optimizer_m.step()\n",
    "        if i%100==0:\n",
    "            print('-'*30)\n",
    "            acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "            print('iter {} loss {} loss_CE {} loss_w2v {} loss_prune: {} loss_pmp: {}'.format(i,loss.item(),loss_CE.item(),loss_w2v.item(),loss_prune.item(),loss_pmp.item()))\n",
    "            print('entropy {} entropy_A_p {}'.format(entropy,entropy_A_p))\n",
    "            print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))  \n",
    "            print('acc_zs {}'.format(acc_zs))\n",
    "    os.mkdir(save_dir_phase_1)\n",
    "    torch.save(model.state_dict(), save_dir_phase_1+'model_phase_1')\n",
    "else:\n",
    "    print('-'*30)\n",
    "    model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "    print(\"Load checkpoint\")\n",
    "    acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "    print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))\n",
    "    print('acc_zs {}'.format(acc_zs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Feature Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "normalize V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.88995361328125 acc_novel 0.2082993984222412 H 0.3375848739675165\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del optimizer_m\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False,margin=1)\n",
    "\n",
    "print('-'*30)\n",
    "model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "print(\"Load checkpoint\")\n",
    "acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))\n",
    "\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.#0.#\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Embed\n",
      "Inherit V\n",
      "Repurpose W_1\n",
      "Inherit W_1\n",
      "Using OMP\n",
      "nearest neighbour k 5\n",
      "T 10\n",
      "n_comp 50\n"
     ]
    }
   ],
   "source": [
    "from core.Composer_AttEmb import Composer_AttEmb\n",
    "\n",
    "pGenerator = Composer_AttEmb(device=device,base_model = model, k=5,n_comp=50,T=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "To prevent seen class bias, we add a fix margin of size 1 to unseen class scores following the DAZLE work.\n",
    "Learning this margin as a function of input could further improve performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.Margin import Margin\n",
    "\n",
    "margin_model = Margin(base_model=model,second_order=True,activate=True)\n",
    "print(\"No hidden layer\")\n",
    "\n",
    "lr_p = 0.001\n",
    "weight_decay_p = 0.01#1#\n",
    "momentum_p = 0.0#0.#\n",
    "\n",
    "optimizer_p_m  = optim.RMSprop( margin_model.parameters() ,lr=lr_p,weight_decay=weight_decay_p, momentum=momentum_p)\n",
    "\n",
    "margin_model.linear1.weight.data  *= 0\n",
    "margin_model.linear1.bias.data  *= 0\n",
    "margin_model.linear1.bias.data  += 1\n",
    "\n",
    "print(margin_model.linear1.weight)\n",
    "print(margin_model.linear1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_model.linear1.weight.requires_grad = False\n",
    "margin_model.linear1.bias.requires_grad = False\n",
    "model.W_1.requires_grad = True\n",
    "model.V.requires_grad = True\n",
    "model.W_2.requires_grad = False\n",
    "model.W_3.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Optimize critic\n",
      "------------------------------------------------------------\n",
      "------------------------------\n",
      "0\n",
      "bias_seen 0 bias_unseen 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/Composer_AttEmb.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  categorical_dis = torch.distributions.categorical.Categorical(probs= torch.tensor(multinomial_prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'iter': 0, 'loss_s_u': 2.1748783588409424, 'acc_seen': 0.7501376867294312, 'acc_novel': 0.587559700012207, 'H': 0.6589691787560048, 'acc_zs': 0.6618930697441101}\n",
      "------------------------------\n",
      "50\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 50, 'loss_s_u': 1.781742811203003, 'acc_seen': 0.7449354529380798, 'acc_novel': 0.6071327328681946, 'H': 0.6690116698263989, 'acc_zs': 0.6966259479522705}\n",
      "------------------------------\n",
      "100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 100, 'loss_s_u': 1.8810646533966064, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "150\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 150, 'loss_s_u': 1.9518096446990967, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 200, 'loss_s_u': 1.826515555381775, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "250\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 250, 'loss_s_u': 1.8032861948013306, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 300, 'loss_s_u': 1.6156821250915527, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "350\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 350, 'loss_s_u': 2.0459206104278564, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 400, 'loss_s_u': 1.5384223461151123, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "450\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 450, 'loss_s_u': 1.4248316287994385, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 500, 'loss_s_u': 1.6171464920043945, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "550\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 550, 'loss_s_u': 1.6318449974060059, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 600, 'loss_s_u': 1.8012025356292725, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "650\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 650, 'loss_s_u': 1.6190139055252075, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 700, 'loss_s_u': 1.8563525676727295, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "750\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 750, 'loss_s_u': 1.6899333000183105, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 800, 'loss_s_u': 1.7416565418243408, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "850\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 850, 'loss_s_u': 1.5669257640838623, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 900, 'loss_s_u': 1.4835326671600342, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n",
      "------------------------------\n",
      "950\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 950, 'loss_s_u': 1.6980056762695312, 'acc_seen': 0.749400794506073, 'acc_novel': 0.6296268701553345, 'H': 0.6843124163903286, 'acc_zs': 0.7190256118774414}\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_performance = [0,0,0,0]\n",
    "\n",
    "lamb = 1#0.7\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(0,1000):\n",
    "    \n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "\n",
    "    Hs_seen = model.extract_attention(batch_feature)['Hs']#.detach()\n",
    "\n",
    "    ### random mini-batch from generator ###\n",
    "    unseen_labels = model.unseenclass[np.random.randint(model.unseenclass.size(0),size=batch_label.size(0))]#model.unseenclass#\n",
    "    att_unseen = model.att[unseen_labels]\n",
    "    Hs_unseen = pGenerator.compose(Hs=Hs_seen,labels_s=batch_label,labels_t=unseen_labels)\n",
    "    \n",
    "    if Hs_unseen.size(0) == 0:\n",
    "        continue\n",
    "\n",
    "    ## seen reinforcement\n",
    "    optimizer_m.zero_grad()\n",
    "    optimizer_p_m.zero_grad()\n",
    "    \n",
    "    \n",
    "    out_package_seen = margin_model.compute_attribute_embed(Hs_seen)\n",
    "\n",
    "    in_package_seen = out_package_seen\n",
    "    in_package_seen['batch_label'] = batch_label\n",
    "    \n",
    "    loss_CE_seen = margin_model.compute_aug_cross_entropy(in_package_seen,is_conservative=True)\n",
    "\n",
    "    out_package_unseen = margin_model.compute_attribute_embed(Hs_unseen)\n",
    "\n",
    "    in_package_unseen = out_package_unseen\n",
    "    in_package_unseen['batch_label'] = unseen_labels\n",
    "    \n",
    "    loss_pmp = model.compute_loss_Laplace(in_package_unseen)\n",
    "    \n",
    "    loss_CE_unseen = margin_model.compute_aug_cross_entropy(in_package_unseen)\n",
    "    \n",
    "    if lamb == -1:\n",
    "        lamb = loss_CE_seen.item()/loss_CE_unseen.item()\n",
    "        print(\"lamb:\",lamb)\n",
    "    \n",
    "    loss_s_u =  loss_CE_seen +  lamb*loss_CE_unseen\n",
    "    \n",
    "    loss_s_u.backward()\n",
    "    optimizer_p_m.step()\n",
    "    optimizer_m.step()\n",
    "    \n",
    "    if i%50==0:\n",
    "        print('-'*30)\n",
    "        print(i)\n",
    "        bias = 0\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,margin_model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss_s_u':loss_s_u.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)\n",
    "        \n",
    "        \n",
    "        tic = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
