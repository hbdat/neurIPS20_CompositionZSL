{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepFashion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-3])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.DeepFashionDataLoader import DeepFashionDataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path_AoA,save_NFS\n",
    "#from core.Scheduler import Scheduler\n",
    "import importlib\n",
    "import pdb\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.helper_func\n",
    "reload(core.helper_func)\n",
    "from core.helper_func import next_unseen_batch,compute_mean_real_unseen,compute_mean_real_seen,compute_generated_quality,evaluate_quality,evaluate_quality_omni,selective_sample,eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k,get_attr_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(critic_iter=5, gen_hidden=1024, lambda1=10, normalize_V=False, nz=312)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--critic_iter', type=int, default=5, help='critic iteration, following WGAN-GP')\n",
    "parser.add_argument('--lambda1', type=float, default=10, help='gradient penalty regularizer, following WGAN-GP')\n",
    "parser.add_argument('--nz', type=int, default=312, help='size of the latent z vector')\n",
    "parser.add_argument('--normalize_V', type=bool, default=False, help='normalize_V')\n",
    "parser.add_argument('--gen_hidden', type=int, default=1024, help='gen_hidden')\n",
    "opt = parser.parse_known_args()[0] #omit unknown arguments\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path  = save_NFS+'results/Relase_DeepFashion_Composer_timing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "DeepFashion\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/DeepFashion/feature_map_ResNet_101_DeepFashion_sep_seen_samples.hdf5\n",
      "Finish loading data in  413.983996\n",
      "Balance dataloader\n",
      "Partition size 10000\n",
      "Excluding non-sample classes\n",
      "------------------------------\n",
      "DeepFashion\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/_hl/dataset.py:313: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"Use dataset[()] instead.\", H5pyDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "idx_GPU = 7\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "dataloader = DeepFashionDataLoader(NFS_path_AoA,device,is_balance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_att = dataloader.att.clone().detach()\n",
    "full_init_w2v_att = dataloader.w2v_att.clone().detach()\n",
    "\n",
    "n_attr = 300\n",
    "attr_entropy = get_attr_entropy(full_att.cpu().numpy())\n",
    "idx_attr_dis = np.argsort(attr_entropy)[:n_attr]\n",
    "init_w2v_att = full_init_w2v_att[idx_attr_dis]\n",
    "att = full_att[:,idx_attr_dis]\n",
    "att = F.normalize(att,dim=1)\n",
    "\n",
    "dataloader.att = att\n",
    "dataloader.w2v_att = init_w2v_att"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrain DAZLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomize seed 214\n",
      "seeker  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = F.normalize(torch.tensor(att)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print('Randomize seed {}'.format(seed))\n",
    "#%%\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att#dataloader.normalize_att#\n",
    "normalize_att = None#dataloader.normalize_att\n",
    "#assert (att.min().item() == 0 and att.max().item() == 1)\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_1 = 0.0\n",
    "lambda_2 = 0.0\n",
    "lambda_3 = 0.0\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = True\n",
    "\n",
    "dataloader.seeker[:] = 0\n",
    "print('seeker ',dataloader.seeker)\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1#unseenclass.size(0)/(seenclass.size(0)+unseenclass.size(0))\n",
    "report_interval = niters//nepoches#10000//batch_size#\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=False,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_1 0.0\n",
      "lambda_2 0.0\n",
      "lambda_3 0.0\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.9 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.9#0.#\n",
    "\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "#%%\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_1 {}'.format(lambda_1))\n",
    "print('lambda_2 {}'.format(lambda_2))\n",
    "print('lambda_3 {}'.format(lambda_3))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_phase_1 = save_path+'phase_1_checkpoint/'\n",
    "run_phase_1 = False\n",
    "if not os.path.isdir(save_dir_phase_1):\n",
    "    run_phase_1 = True\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.409391313791275 acc_novel 0.08848311007022858 H 0.14551547516353464\n"
     ]
    }
   ],
   "source": [
    "if run_phase_1:\n",
    "    #%% phase 1 convergence of attention model\n",
    "    for i in range(0,2000):\n",
    "        tic = time.time()\n",
    "        model.train()\n",
    "        optimizer_m.zero_grad()\n",
    "        batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "        out_package = model(batch_feature)\n",
    "\n",
    "        in_package = out_package\n",
    "        in_package['batch_label'] = batch_label\n",
    "        \n",
    "        out_package=model.compute_loss(in_package)\n",
    "        loss,loss_CE,loss_w2v,loss_prune,loss_pmp = out_package['loss'],out_package['loss_CE'],out_package['loss_w2v'],out_package['loss_prune'],out_package['loss_pmp']\n",
    "        entropy = out_package['entropy']\n",
    "        entropy_A_p = out_package['entropy_A_p']\n",
    "        loss.backward()\n",
    "        optimizer_m.step()\n",
    "        #print(\"Elapse time training {}\".format(time.time()-tic))\n",
    "        if i%100==0:\n",
    "            print('-'*30)\n",
    "            acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "            print('iter {} loss {} loss_CE {} loss_w2v {} loss_prune: {} loss_pmp: {}'.format(i,loss.item(),loss_CE.item(),loss_w2v.item(),loss_prune.item(),loss_pmp.item()))\n",
    "            print('entropy {} entropy_A_p {}'.format(entropy,entropy_A_p))\n",
    "            print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))  \n",
    "            print('acc_zs {}'.format(acc_zs))\n",
    "    os.mkdir(save_dir_phase_1)\n",
    "    torch.save(model.state_dict(), save_dir_phase_1+'model_phase_1')\n",
    "else:\n",
    "    print('-'*30)\n",
    "    model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "    print(\"Load checkpoint\")\n",
    "    acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "    print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add bias\n",
      "bias_seen -2 bias_unseen 2\n",
      "acc_seen 0.34127628803253174 acc_novel 0.22246943414211273 H 0.269353858302795\n"
     ]
    }
   ],
   "source": [
    "print(\"Add bias\")\n",
    "test_bias = 2\n",
    "acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-test_bias,bias_unseen=test_bias)\n",
    "print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Feature Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.409391313791275 acc_novel 0.08848311007022858 H 0.14551547516353464\n"
     ]
    }
   ],
   "source": [
    "del model\n",
    "del optimizer_m\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False,margin=1)\n",
    "\n",
    "print('-'*30)\n",
    "model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "print(\"Load checkpoint\")\n",
    "acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))\n",
    "\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.#0.#\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Embed\n",
      "Inherit V\n",
      "Repurpose W_1\n",
      "Inherit W_1\n",
      "Using OMP\n",
      "nearest neighbour k 5\n",
      "T 5\n",
      "n_comp 1\n"
     ]
    }
   ],
   "source": [
    "from core.Composer_AttEmb import Composer_AttEmb\n",
    "\n",
    "pGenerator = Composer_AttEmb(device=device,base_model = model, k=5,n_comp=1,T=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "To prevent seen class bias, we add a fix margin of size 1 to unseen class scores following the DAZLE work.\n",
    "Learning this margin as a function of input could further improve performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.Margin\n",
    "reload(core.Margin)\n",
    "from core.Margin import Margin\n",
    "\n",
    "margin_model = Margin(base_model=model,second_order=True,activate=True)\n",
    "print(\"No hidden layer\")\n",
    "\n",
    "lr_p = 0.001\n",
    "weight_decay_p = 0.01#0.000#0.#\n",
    "momentum_p = 0.0#0.#\n",
    "optimizer_p_m  = optim.RMSprop( margin_model.parameters() ,lr=lr_p,weight_decay=weight_decay_p, momentum=momentum_p)\n",
    "\n",
    "margin_model.linear1.weight.data  *= 0\n",
    "margin_model.linear1.bias.data  *= 0\n",
    "margin_model.linear1.bias.data  += 1\n",
    "\n",
    "print(margin_model.linear1.weight)\n",
    "print(margin_model.linear1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_model.linear1.weight.requires_grad = False\n",
    "margin_model.linear1.bias.requires_grad = False\n",
    "model.W_1.requires_grad = True\n",
    "model.V.requires_grad = True\n",
    "model.W_2.requires_grad = False\n",
    "model.W_3.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 35.505596999999966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/Composer_AttEmb.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  categorical_dis = torch.distributions.categorical.Categorical(probs= torch.tensor(multinomial_prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "0\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss_s_u': 4.484494209289551, 'acc_seen': 0.3416859805583954, 'acc_novel': 0.22585919499397278, 'H': 0.27195340154036624, 'acc_zs': 0.3678618371486664}\n",
      "------------------------------\n",
      "100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 100, 'loss_s_u': 3.2965941429138184, 'acc_seen': 0.3384915590286255, 'acc_novel': 0.26523396372795105, 'H': 0.2974181296152203, 'acc_zs': 0.4005167484283447}\n",
      "------------------------------\n",
      "200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 200, 'loss_s_u': 3.277092456817627, 'acc_seen': 0.33322349190711975, 'acc_novel': 0.28073975443840027, 'H': 0.30473837594657727, 'acc_zs': 0.40838345885276794}\n",
      "------------------------------\n",
      "300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 300, 'loss_s_u': 2.9562084674835205, 'acc_seen': 0.3277011513710022, 'acc_novel': 0.29004111886024475, 'H': 0.3077231822256744, 'acc_zs': 0.4146690368652344}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 38.308009000000084\n",
      "------------------------------\n",
      "400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 400, 'loss_s_u': 3.330197811126709, 'acc_seen': 0.327499657869339, 'acc_novel': 0.2961740493774414, 'H': 0.31105014918484686, 'acc_zs': 0.4186627268791199}\n",
      "------------------------------\n",
      "500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 500, 'loss_s_u': 2.9055793285369873, 'acc_seen': 0.3278971314430237, 'acc_novel': 0.30169737339019775, 'H': 0.31425211795565444, 'acc_zs': 0.41587504744529724}\n",
      "------------------------------\n",
      "600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 600, 'loss_s_u': 2.8245229721069336, 'acc_seen': 0.3294123709201813, 'acc_novel': 0.302612841129303, 'H': 0.31544442078203383, 'acc_zs': 0.42226481437683105}\n",
      "------------------------------\n",
      "700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 700, 'loss_s_u': 2.4778318405151367, 'acc_seen': 0.3294123709201813, 'acc_novel': 0.302612841129303, 'H': 0.31544442078203383, 'acc_zs': 0.42226481437683105}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 38.312116999999944\n",
      "------------------------------\n",
      "800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 800, 'loss_s_u': 2.546644449234009, 'acc_seen': 0.3294123709201813, 'acc_novel': 0.302612841129303, 'H': 0.31544442078203383, 'acc_zs': 0.42226481437683105}\n",
      "------------------------------\n",
      "900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 900, 'loss_s_u': 3.1284875869750977, 'acc_seen': 0.3241635262966156, 'acc_novel': 0.3085286617279053, 'H': 0.31615291240301285, 'acc_zs': 0.4226159453392029}\n",
      "------------------------------\n",
      "1000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1000, 'loss_s_u': 3.2353029251098633, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1100, 'loss_s_u': 4.226963996887207, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 38.65221799999972\n",
      "------------------------------\n",
      "1200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1200, 'loss_s_u': 3.4580769538879395, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1300, 'loss_s_u': 3.1671109199523926, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1400, 'loss_s_u': 3.095311164855957, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1500, 'loss_s_u': 2.7819719314575195, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 37.77503499999966\n",
      "------------------------------\n",
      "1600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1600, 'loss_s_u': 2.9246630668640137, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1700, 'loss_s_u': 3.158170223236084, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1800, 'loss_s_u': 2.6355583667755127, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "1900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1900, 'loss_s_u': 2.592076539993286, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 32.42033300000003\n",
      "------------------------------\n",
      "2000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2000, 'loss_s_u': 2.4416041374206543, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2100, 'loss_s_u': 2.75518798828125, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2200, 'loss_s_u': 3.1487908363342285, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2300, 'loss_s_u': 2.7678070068359375, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 36.060771999999815\n",
      "------------------------------\n",
      "2400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2400, 'loss_s_u': 2.9503238201141357, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2500, 'loss_s_u': 3.553210735321045, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2600, 'loss_s_u': 2.1200385093688965, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2700, 'loss_s_u': 2.4160337448120117, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 41.961501000000226\n",
      "------------------------------\n",
      "2800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2800, 'loss_s_u': 3.64583420753479, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "2900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2900, 'loss_s_u': 2.6668286323547363, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3000, 'loss_s_u': 3.0234808921813965, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3100, 'loss_s_u': 2.9729809761047363, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 34.53494699999919\n",
      "------------------------------\n",
      "3200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3200, 'loss_s_u': 3.311781406402588, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3300, 'loss_s_u': 3.3835537433624268, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3400, 'loss_s_u': 2.8981847763061523, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3500, 'loss_s_u': 3.0351552963256836, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "load data from hdf\n",
      "1..10..11..13..14..16..17..18..20..21..22..23..24..25..27..28..3..31..32..33..34..35..36..37..39..4..40..41..43..46..47..5..6..7..8..9..\n",
      "Elapsed time 35.30489800000032\n",
      "------------------------------\n",
      "3600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3600, 'loss_s_u': 2.644737720489502, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3700, 'loss_s_u': 2.450486183166504, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3800, 'loss_s_u': 3.369454860687256, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n",
      "------------------------------\n",
      "3900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3900, 'loss_s_u': 3.382406711578369, 'acc_seen': 0.3255787789821625, 'acc_novel': 0.30893442034721375, 'H': 0.3170382947699581, 'acc_zs': 0.42185649275779724}\n"
     ]
    }
   ],
   "source": [
    "seed = 113\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_performance = [0,0,0,0]\n",
    "\n",
    "lamb = 1#0.7\n",
    "\n",
    "\n",
    "for i in range(0,4000):\n",
    "    tic = time.time()\n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "\n",
    "    Hs_seen = model.extract_attention(batch_feature)['Hs']\n",
    "\n",
    "    ### random mini-batch from generator ###\n",
    "    unseen_labels = model.unseenclass[np.random.randint(model.unseenclass.size(0),size=batch_label.size(0))]\n",
    "    att_unseen = model.att[unseen_labels]\n",
    "    Hs_unseen = pGenerator.compose(Hs=Hs_seen,labels_s=batch_label,labels_t=unseen_labels)\n",
    "    ### random mini-batch from generator ###\n",
    "\n",
    "    if Hs_unseen.size(0) == 0:\n",
    "        continue\n",
    "\n",
    "    ## seen reinforcement\n",
    "    optimizer_m.zero_grad()\n",
    "    optimizer_p_m.zero_grad()\n",
    "    \n",
    "    \n",
    "    out_package_seen = margin_model.compute_attribute_embed(Hs_seen)\n",
    "\n",
    "    in_package_seen = out_package_seen\n",
    "    in_package_seen['batch_label'] = batch_label\n",
    "    \n",
    "    loss_CE_seen = margin_model.compute_aug_cross_entropy(in_package_seen,is_conservative=True)\n",
    "\n",
    "    out_package_unseen = margin_model.compute_attribute_embed(Hs_unseen)\n",
    "\n",
    "    in_package_unseen = out_package_unseen\n",
    "    in_package_unseen['batch_label'] = unseen_labels\n",
    "    \n",
    "    loss_pmp = model.compute_loss_Laplace(in_package_unseen)\n",
    "    \n",
    "    loss_CE_unseen = margin_model.compute_aug_cross_entropy(in_package_unseen)\n",
    "    \n",
    "    if lamb == -1:\n",
    "        lamb = loss_CE_seen.item()/loss_CE_unseen.item()\n",
    "        print(\"lamb:\",lamb)\n",
    "    \n",
    "    loss_s_u =  loss_CE_seen + lamb*loss_CE_unseen#0.4*loss_pmp # \n",
    "\n",
    "    loss_s_u.backward()\n",
    "    optimizer_p_m.step()\n",
    "    optimizer_m.step()\n",
    "    \n",
    "    if i%100==0:\n",
    "        print('-'*30)\n",
    "        print(i)\n",
    "        bias = 0\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,margin_model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss_s_u':loss_s_u.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)\n",
    "        \n",
    "        \n",
    "        tic = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
