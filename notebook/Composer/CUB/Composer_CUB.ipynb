{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os,sys\n",
    "pwd = os.getcwd()\n",
    "parent = '/'.join(pwd.split('/')[:-3])\n",
    "sys.path.insert(0,parent)\n",
    "os.chdir(parent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "print('-'*30)\n",
    "print(os.getcwd())\n",
    "print('-'*30)\n",
    "#%%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  \n",
    "import pandas as pd\n",
    "from core.DAZLE import DAZLE\n",
    "from core.CUBDataLoader import CUBDataLoader\n",
    "from core.helper_func import eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k#,get_attribute_attention_stats\n",
    "from global_setting import NFS_path_AoA,save_NFS\n",
    "#from core.Scheduler import Scheduler\n",
    "import importlib\n",
    "import pdb\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "from importlib import reload\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import core.helper_func\n",
    "reload(core.helper_func)\n",
    "from core.helper_func import next_unseen_batch,compute_mean_real_unseen,compute_mean_real_seen,compute_generated_quality,evaluate_quality,evaluate_quality_omni,selective_sample,eval_zs_gzsl,visualize_attention,eval_zs_gzsl_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(critic_iter=5, gen_hidden=1024, lambda1=10, normalize_V=False, nz=312)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--critic_iter', type=int, default=5, help='critic iteration, following WGAN-GP')\n",
    "parser.add_argument('--lambda1', type=float, default=10, help='gradient penalty regularizer, following WGAN-GP')\n",
    "parser.add_argument('--nz', type=int, default=312, help='size of the latent z vector')\n",
    "parser.add_argument('--normalize_V', type=bool, default=False, help='normalize_V')\n",
    "parser.add_argument('--gen_hidden', type=int, default=1024, help='gen_hidden')\n",
    "opt = parser.parse_known_args()[0] #omit unknown arguments\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path  = save_NFS+'results/Release_CUB_Composer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "CUB\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Balance dataloader\n",
      "_____\n",
      "/home/project_amadeus/mnt/raptor/hbdat/Attention_over_attention/data/CUB/feature_map_ResNet_101_CUB.hdf5\n",
      "Expert Attr\n",
      "Finish loading data in  54.616477\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "idx_GPU = 4\n",
    "device = torch.device(\"cuda:{}\".format(idx_GPU) if torch.cuda.is_available() else \"cpu\")\n",
    "#%%\n",
    "dataloader = CUBDataLoader(NFS_path_AoA,device,is_unsupervised_attr=False,is_balance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained DAZLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomize seed 214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:54: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.init_w2v_att = F.normalize(torch.tensor(init_w2v_att))\n",
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/DAZLE.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.att = F.normalize(torch.tensor(att)).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "print('Randomize seed {}'.format(seed))\n",
    "#%%\n",
    "\n",
    "batch_size = 50\n",
    "nepoches = 20\n",
    "niters = dataloader.ntrain * nepoches//batch_size\n",
    "dim_f = 2048\n",
    "dim_v = 300\n",
    "init_w2v_att = dataloader.w2v_att\n",
    "att = dataloader.att#dataloader.normalize_att#\n",
    "normalize_att = dataloader.normalize_att\n",
    "#assert (att.min().item() == 0 and att.max().item() == 1)\n",
    "\n",
    "trainable_w2v = True\n",
    "lambda_1 = 0.0\n",
    "lambda_2 = 0.0\n",
    "lambda_3 = 0.0\n",
    "bias = 0\n",
    "prob_prune = 0\n",
    "uniform_att_1 = False\n",
    "uniform_att_2 = True\n",
    "\n",
    "seenclass = dataloader.seenclasses\n",
    "unseenclass = dataloader.unseenclasses\n",
    "desired_mass = 1#unseenclass.size(0)/(seenclass.size(0)+unseenclass.size(0))\n",
    "report_interval = niters//nepoches#10000//batch_size#\n",
    "\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=False,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "learing rate 0.0001\n",
      "trainable V True\n",
      "lambda_1 0.0\n",
      "lambda_2 0.0\n",
      "lambda_3 0.0\n",
      "optimized seen only\n",
      "optimizer: RMSProp with momentum = 0.9 and weight_decay = 0.0001\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "lr = 0.0001\n",
    "weight_decay = 0.0001#0.000#0.#\n",
    "momentum = 0.9#0.#\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=momentum)\n",
    "\n",
    "#%%\n",
    "print('-'*30)\n",
    "print('learing rate {}'.format(lr))\n",
    "print('trainable V {}'.format(trainable_w2v))\n",
    "print('lambda_1 {}'.format(lambda_1))\n",
    "print('lambda_2 {}'.format(lambda_2))\n",
    "print('lambda_3 {}'.format(lambda_3))\n",
    "print('optimized seen only')\n",
    "print('optimizer: RMSProp with momentum = {} and weight_decay = {}'.format(momentum,weight_decay))\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir_phase_1 = save_path+'phase_1_checkpoint/'\n",
    "run_phase_1 = False\n",
    "if not os.path.isdir(save_dir_phase_1):\n",
    "    run_phase_1 = True\n",
    "    os.mkdir(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.6486324071884155 acc_novel 0.4058607518672943 H 0.4993004159509684\n",
      "acc_zs 0.6082878708839417\n"
     ]
    }
   ],
   "source": [
    "if run_phase_1:\n",
    "    #%% phase 1 convergence of attention model\n",
    "    for i in range(0,2000):\n",
    "        model.train()\n",
    "        optimizer_m.zero_grad()\n",
    "        batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "        out_package = model(batch_feature)\n",
    "\n",
    "        in_package = out_package\n",
    "        in_package['batch_label'] = batch_label\n",
    "        \n",
    "        out_package=model.compute_loss(in_package)\n",
    "        loss,loss_CE,loss_w2v,loss_prune,loss_pmp = out_package['loss'],out_package['loss_CE'],out_package['loss_w2v'],out_package['loss_prune'],out_package['loss_pmp']\n",
    "        entropy = out_package['entropy']\n",
    "        entropy_A_p = out_package['entropy_A_p']\n",
    "        loss.backward()\n",
    "        optimizer_m.step()\n",
    "        if i%100==0:\n",
    "            print('-'*30)\n",
    "            acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "            print('iter {} loss {} loss_CE {} loss_w2v {} loss_prune: {} loss_pmp: {}'.format(i,loss.item(),loss_CE.item(),loss_w2v.item(),loss_prune.item(),loss_pmp.item()))\n",
    "            print('entropy {} entropy_A_p {}'.format(entropy,entropy_A_p))\n",
    "            print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))  \n",
    "            print('acc_zs {}'.format(acc_zs))\n",
    "    os.mkdir(save_dir_phase_1)\n",
    "    torch.save(model.state_dict(), save_dir_phase_1+'model_phase_1')\n",
    "else:\n",
    "    print('-'*30)\n",
    "    model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "    print(\"Load checkpoint\")\n",
    "    acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "    print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))\n",
    "    print('acc_zs {}'.format(acc_zs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add bias\n",
      "bias_seen -1 bias_unseen 1\n",
      "acc_seen 0.5697115063667297 acc_novel 0.5140376687049866 H 0.5404445628256751\n"
     ]
    }
   ],
   "source": [
    "print(\"Add bias\")\n",
    "test_bias = 1\n",
    "acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-test_bias,bias_unseen=test_bias)\n",
    "print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense Feature Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Configuration\n",
      "loss_type CE\n",
      "no constraint V\n",
      "normalize F\n",
      "training to exclude unseen class [seen upperbound]\n",
      "Init word2vec\n",
      "Linear model\n",
      "loss_att BCEWithLogitsLoss()\n",
      "Bilinear attention module\n",
      "******************************\n",
      "Measure w2v deviation\n",
      "WARNING: UNIFORM ATTENTION LEVEL 2\n",
      "new Laplacian smoothing with desire mass 1 4\n",
      "Compute Pruning loss 0\n",
      "Second layer attenion conditioned on image features\n",
      "------------------------------\n",
      "No sigmoid on attr score\n",
      "------------------------------\n",
      "Load checkpoint\n",
      "bias_seen 0 bias_unseen 0\n",
      "acc_seen 0.6486324071884155 acc_novel 0.4058607518672943 H 0.4993004159509684\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del model\n",
    "    del optimizer_m\n",
    "    del progressive_model\n",
    "except:\n",
    "    pass\n",
    "model = DAZLE(dim_f,dim_v,init_w2v_att,att,normalize_att,\n",
    "            seenclass,unseenclass,\n",
    "            lambda_1,lambda_2,lambda_3,\n",
    "            device,\n",
    "            trainable_w2v,normalize_V=opt.normalize_V,normalize_F=True,is_conservative=True,\n",
    "            uniform_att_1=uniform_att_1,uniform_att_2=uniform_att_2,\n",
    "            prob_prune=prob_prune,desired_mass=desired_mass, is_conv=False,\n",
    "            is_bias=False,margin=1)\n",
    "\n",
    "print('-'*30)\n",
    "model.load_state_dict(torch.load(save_dir_phase_1+'model_phase_1'))\n",
    "print(\"Load checkpoint\")\n",
    "acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "print('acc_seen {} acc_novel {} H {}'.format(acc_seen, acc_novel, H))\n",
    "\n",
    "optimizer_m  = optim.RMSprop( model.parameters() ,lr=lr,weight_decay=weight_decay, momentum=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute Embed\n",
      "Inherit V\n",
      "Repurpose W_1\n",
      "Inherit W_1\n",
      "Using OMP\n",
      "nearest neighbour k 5\n",
      "T 5\n",
      "n_comp 50\n"
     ]
    }
   ],
   "source": [
    "from core.Composer_AttEmb import Composer_AttEmb\n",
    "\n",
    "pGenerator = Composer_AttEmb(device=device,base_model = model, k=5,n_comp=50,T=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark\n",
    "To prevent seen class bias, we add a fix margin of size 1 to unseen class scores following the DAZLE work.\n",
    "Learning this margin as a function of input could further improve performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.Margin import Margin\n",
    "\n",
    "margin_model = Margin(base_model=model,second_order=True,activate=True)\n",
    "print(\"No hidden layer\")\n",
    "\n",
    "lr_p = 0.001\n",
    "weight_decay_p = 0.01#0.000#0.#\n",
    "momentum_p = 0.0#0.#\n",
    "\n",
    "optimizer_p_m  = optim.RMSprop( margin_model.parameters() ,lr=lr_p,weight_decay=weight_decay_p, momentum=momentum_p)\n",
    "\n",
    "margin_model.linear1.weight.data  *= 0\n",
    "margin_model.linear1.bias.data  *= 0\n",
    "margin_model.linear1.bias.data  += 1\n",
    "\n",
    "print(margin_model.linear1.weight)\n",
    "print(margin_model.linear1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_model.linear1.weight.requires_grad = False\n",
    "margin_model.linear1.bias.requires_grad = False\n",
    "model.W_1.requires_grad = True\n",
    "model.V.requires_grad = True\n",
    "model.W_2.requires_grad = False\n",
    "model.W_3.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/project_amadeus/home/hbdat/[RELEASE]_CompositionalZSL/core/Composer_AttEmb.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  categorical_dis = torch.distributions.categorical.Categorical(probs= torch.tensor(multinomial_prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "0\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 0, 'loss_s_u': 3.9716551303863525, 'acc_seen': 0.4706075191497803, 'acc_novel': 0.5857439041137695, 'H': 0.5219011012840078, 'acc_zs': 0.6163476705551147}\n",
      "------------------------------\n",
      "100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 100, 'loss_s_u': 3.504664421081543, 'acc_seen': 0.5058590769767761, 'acc_novel': 0.6201344728469849, 'H': 0.557197955680928, 'acc_zs': 0.6583583950996399}\n",
      "------------------------------\n",
      "200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 200, 'loss_s_u': 3.4658613204956055, 'acc_seen': 0.5164749026298523, 'acc_novel': 0.6242353916168213, 'H': 0.5652651943784203, 'acc_zs': 0.6644651889801025}\n",
      "------------------------------\n",
      "300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 300, 'loss_s_u': 3.8994407653808594, 'acc_seen': 0.5218608975410461, 'acc_novel': 0.6317852139472961, 'H': 0.5715860271540985, 'acc_zs': 0.6743836402893066}\n",
      "------------------------------\n",
      "400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 400, 'loss_s_u': 3.1986780166625977, 'acc_seen': 0.5218608975410461, 'acc_novel': 0.6317852139472961, 'H': 0.5715860271540985, 'acc_zs': 0.6743836402893066}\n",
      "------------------------------\n",
      "500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 500, 'loss_s_u': 3.5969393253326416, 'acc_seen': 0.5245934128761292, 'acc_novel': 0.6338348388671875, 'H': 0.5740633152217105, 'acc_zs': 0.6800886392593384}\n",
      "------------------------------\n",
      "600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 600, 'loss_s_u': 2.895303249359131, 'acc_seen': 0.5331999063491821, 'acc_novel': 0.6316177248954773, 'H': 0.5782510544640179, 'acc_zs': 0.6795722246170044}\n",
      "------------------------------\n",
      "700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 700, 'loss_s_u': 2.915909767150879, 'acc_seen': 0.5340398550033569, 'acc_novel': 0.6393716931343079, 'H': 0.5819781931354192, 'acc_zs': 0.6829872727394104}\n",
      "------------------------------\n",
      "800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 800, 'loss_s_u': 3.309427261352539, 'acc_seen': 0.5390872955322266, 'acc_novel': 0.6373987197875977, 'H': 0.584135378664215, 'acc_zs': 0.6806696057319641}\n",
      "------------------------------\n",
      "900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 900, 'loss_s_u': 2.886704444885254, 'acc_seen': 0.5389896035194397, 'acc_novel': 0.637615442276001, 'H': 0.5841689964840803, 'acc_zs': 0.6805531978607178}\n",
      "------------------------------\n",
      "1000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1000, 'loss_s_u': 3.098587989807129, 'acc_seen': 0.5371484756469727, 'acc_novel': 0.6434268951416016, 'H': 0.5855039575909823, 'acc_zs': 0.6873201727867126}\n",
      "------------------------------\n",
      "1100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1100, 'loss_s_u': 2.9670205116271973, 'acc_seen': 0.5454174280166626, 'acc_novel': 0.6363180875778198, 'H': 0.5873716582895199, 'acc_zs': 0.6845948100090027}\n",
      "------------------------------\n",
      "1200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1200, 'loss_s_u': 3.1852669715881348, 'acc_seen': 0.5440003871917725, 'acc_novel': 0.6430267095565796, 'H': 0.589382971849023, 'acc_zs': 0.6899257898330688}\n",
      "------------------------------\n",
      "1300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1300, 'loss_s_u': 2.5717639923095703, 'acc_seen': 0.5469220876693726, 'acc_novel': 0.6449546813964844, 'H': 0.5919067641161646, 'acc_zs': 0.6905032992362976}\n",
      "------------------------------\n",
      "1400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1400, 'loss_s_u': 2.6620726585388184, 'acc_seen': 0.5483432412147522, 'acc_novel': 0.6435545086860657, 'H': 0.5921460380651776, 'acc_zs': 0.6914535760879517}\n",
      "------------------------------\n",
      "1500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1500, 'loss_s_u': 3.5291213989257812, 'acc_seen': 0.5483432412147522, 'acc_novel': 0.6435545086860657, 'H': 0.5921460380651776, 'acc_zs': 0.6914535760879517}\n",
      "------------------------------\n",
      "1600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1600, 'loss_s_u': 3.1329050064086914, 'acc_seen': 0.5531119108200073, 'acc_novel': 0.6410478949546814, 'H': 0.5938421715266253, 'acc_zs': 0.6892974376678467}\n",
      "------------------------------\n",
      "1700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1700, 'loss_s_u': 2.937641143798828, 'acc_seen': 0.5539976358413696, 'acc_novel': 0.6410419940948486, 'H': 0.594349744238252, 'acc_zs': 0.6892918348312378}\n",
      "------------------------------\n",
      "1800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1800, 'loss_s_u': 2.4929370880126953, 'acc_seen': 0.5539976358413696, 'acc_novel': 0.6410419940948486, 'H': 0.594349744238252, 'acc_zs': 0.6892918348312378}\n",
      "------------------------------\n",
      "1900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 1900, 'loss_s_u': 3.1890368461608887, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2000, 'loss_s_u': 3.0941295623779297, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2100, 'loss_s_u': 3.3600592613220215, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2200, 'loss_s_u': 2.6762242317199707, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2300, 'loss_s_u': 2.5923075675964355, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2400, 'loss_s_u': 2.640725612640381, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2500, 'loss_s_u': 2.735187530517578, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2600, 'loss_s_u': 2.76155424118042, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2700, 'loss_s_u': 2.7166271209716797, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2800, 'loss_s_u': 2.5456106662750244, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "2900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 2900, 'loss_s_u': 3.056436061859131, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "3000\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3000, 'loss_s_u': 3.111154556274414, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "3100\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3100, 'loss_s_u': 2.801487684249878, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "3200\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3200, 'loss_s_u': 2.7927398681640625, 'acc_seen': 0.555447518825531, 'acc_novel': 0.6429372429847717, 'H': 0.5959987272150389, 'acc_zs': 0.6935087442398071}\n",
      "------------------------------\n",
      "3300\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3300, 'loss_s_u': 3.020763397216797, 'acc_seen': 0.5630382299423218, 'acc_novel': 0.6361812949180603, 'H': 0.5973791833564105, 'acc_zs': 0.6921724081039429}\n",
      "------------------------------\n",
      "3400\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3400, 'loss_s_u': 2.8432865142822266, 'acc_seen': 0.5630382299423218, 'acc_novel': 0.6361812949180603, 'H': 0.5973791833564105, 'acc_zs': 0.6921724081039429}\n",
      "------------------------------\n",
      "3500\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3500, 'loss_s_u': 2.848074436187744, 'acc_seen': 0.5630382299423218, 'acc_novel': 0.6361812949180603, 'H': 0.5973791833564105, 'acc_zs': 0.6921724081039429}\n",
      "------------------------------\n",
      "3600\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3600, 'loss_s_u': 3.067054271697998, 'acc_seen': 0.562098503112793, 'acc_novel': 0.6375237703323364, 'H': 0.5974399857940305, 'acc_zs': 0.6934729814529419}\n",
      "------------------------------\n",
      "3700\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3700, 'loss_s_u': 2.967557668685913, 'acc_seen': 0.5647217631340027, 'acc_novel': 0.6363551020622253, 'H': 0.5984022931907634, 'acc_zs': 0.6934006214141846}\n",
      "------------------------------\n",
      "3800\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3800, 'loss_s_u': 2.6336021423339844, 'acc_seen': 0.5654041767120361, 'acc_novel': 0.6368491053581238, 'H': 0.5990038030668378, 'acc_zs': 0.6941498517990112}\n",
      "------------------------------\n",
      "3900\n",
      "bias_seen 0 bias_unseen 0\n",
      "{'iter': 3900, 'loss_s_u': 2.508152961730957, 'acc_seen': 0.5654041767120361, 'acc_novel': 0.6368491053581238, 'H': 0.5990038030668378, 'acc_zs': 0.6941498517990112}\n"
     ]
    }
   ],
   "source": [
    "seed = 214\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "best_performance = [0,0,0,0]\n",
    "\n",
    "lamb = 1#0.5\n",
    "\n",
    "tic = time.time()\n",
    "for i in range(0,4000):\n",
    "    batch_label, batch_feature, batch_att = dataloader.next_batch(batch_size)\n",
    "\n",
    "    Hs_seen = model.extract_attention(batch_feature)['Hs'] #.detach()\n",
    "\n",
    "    ### random mini-batch from generator ###\n",
    "    unseen_labels = model.unseenclass[np.random.randint(model.unseenclass.size(0),size=batch_label.size(0))]\n",
    "    att_unseen = model.att[unseen_labels]\n",
    "    Hs_unseen = pGenerator.compose(Hs=Hs_seen,labels_s=batch_label,labels_t=unseen_labels)\n",
    "    ### random mini-batch from generator ###\n",
    "\n",
    "    if Hs_unseen.size(0) == 0:\n",
    "        continue\n",
    "\n",
    "    ## seen reinforcement\n",
    "    optimizer_m.zero_grad()\n",
    "    optimizer_p_m.zero_grad()\n",
    "    \n",
    "    \n",
    "    out_package_seen = margin_model.compute_attribute_embed(Hs_seen,activate=True)\n",
    "\n",
    "    in_package_seen = out_package_seen\n",
    "    in_package_seen['batch_label'] = batch_label\n",
    "    \n",
    "    loss_CE_seen = margin_model.compute_aug_cross_entropy(in_package_seen,is_conservative=True)\n",
    "\n",
    "    out_package_unseen = margin_model.compute_attribute_embed(Hs_unseen)\n",
    "\n",
    "    in_package_unseen = out_package_unseen\n",
    "    in_package_unseen['batch_label'] = unseen_labels\n",
    "    \n",
    "    \n",
    "    loss_pmp = model.compute_loss_Laplace(in_package_seen)\n",
    "    \n",
    "    loss_CE_unseen = margin_model.compute_aug_cross_entropy(in_package_unseen)\n",
    "    \n",
    "    if lamb == -1:\n",
    "        lamb = loss_CE_seen.item()/loss_CE_unseen.item()\n",
    "        print(\"lamb:\",lamb)\n",
    "        \n",
    "    loss_s_u =   loss_CE_seen + lamb*loss_CE_unseen #0.5*loss_pmp # \n",
    "    \n",
    "    loss_s_u.backward()\n",
    "    optimizer_p_m.step()\n",
    "    optimizer_m.step()\n",
    "    \n",
    "    if i%100==0:\n",
    "        print('-'*30)\n",
    "        print(i)\n",
    "        bias = 0\n",
    "        acc_seen, acc_novel, H, acc_zs = eval_zs_gzsl(dataloader,margin_model,device,bias_seen=-bias,bias_unseen=bias)\n",
    "        \n",
    "        if H > best_performance[2]:\n",
    "            best_performance = [acc_seen, acc_novel, H, acc_zs]\n",
    "        stats_package = {'iter':i, 'loss_s_u':loss_s_u.item(),\n",
    "                         'acc_seen':best_performance[0], 'acc_novel':best_performance[1], 'H':best_performance[2], 'acc_zs':best_performance[3]}\n",
    "        \n",
    "        print(stats_package)\n",
    "        \n",
    "        \n",
    "        tic = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
